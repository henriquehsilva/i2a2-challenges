{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "I2A2 - Bone Age Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN/oqN92x8Ksb/RdZQM6rIS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/henriquehsilva/i2a2-challenges/blob/master/I2A2_Bone_Age_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Bvw4DP4geMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import HTML, display"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EL5RRm6h25nn",
        "colab_type": "text"
      },
      "source": [
        "# I2A2 - Bone Age Regression\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc_Qc37k2oWO",
        "colab_type": "text"
      },
      "source": [
        "## Contexto\n",
        "> ### Criar um modelo para prever a idade √≥ssea de crian√ßas pelo raio-x da m√£o.\n",
        "\n",
        "\n",
        "*   O conjunto de dados de treinamento vem da competi√ß√£o RSNA 2017\n",
        "*   O conjunto de dados de teste √© de um hospital brasileiro\n",
        "\n",
        "**Diretivas prim√°rias:** \n",
        "\n",
        "\n",
        "1. Generalizar o modelo para lidar com a mudan√ßa de distribui√ß√£o entre os conjuntos de dados;\n",
        "2. O conjunto de dados de **TREINAMENTO cont√©m apenas imagens com a m√£o esquerda**, o conjunto de dados de **TESTE cont√©m algumas imagens com as duas m√£os**;\n",
        "3. Ajustar o modelo, para lidar com a vari√¢ncia de resultados entre pacientes de sexos opostos;\n",
        "\n",
        "---\n",
        "## Vis√£o geral\n",
        "\n",
        "Uma das principais preocupa√ß√µes da medicina e da odontologia legal √© a busca\n",
        "de mecanismos que permitam a determina√ß√£o da idade de pessoas e, para o desenvolvimento\n",
        "deste trabalho, escolheu-se a **regi√£o de m√£o e pulso**, para a determina√ß√£o da idade e do\n",
        "desenvolvimento √≥sseo, devido a seq√º√™ncia cronol√≥gica que a mesma apresenta bem como a\n",
        "quantidade de ossos e ep√≠fises em uma √°rea n√£o muito extensa sendo poss√≠vel realizar uma\n",
        "√∫nica tomada radiogr√°fica, evitando-se exposi√ß√µes desnecess√°rias ao paciente. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQhsbJhugnwe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "7ff0a236-88f3-4251-bc01-8760394a31cd"
      },
      "source": [
        "video_ids = ['Zs1oC_upTvM', 'WIk5S0BHotY']\n",
        "\n",
        "def embed_video(video_ids):\n",
        "  videos_str = \"\"\n",
        "  \n",
        "  for video_id in video_ids: \n",
        "    videos_str += (\n",
        "      f\"<iframe width='560' height='315' src='https://www.youtube.com/embed/{video_id}'.format(test)\" \\\n",
        "       \"frameborder='0' allow='accelerometer; autoplay; encrypted-media; gyroscope;\" \\\n",
        "       \" picture-in-picture' allowfullscreen></iframe>\"\n",
        "      )\n",
        "    \n",
        "  return HTML(videos_str)\n",
        "\n",
        "embed_video(video_ids)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width='560' height='315' src='https://www.youtube.com/embed/Zs1oC_upTvM'.format(test)frameborder='0' allow='accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture' allowfullscreen></iframe><iframe width='560' height='315' src='https://www.youtube.com/embed/WIk5S0BHotY'.format(test)frameborder='0' allow='accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture' allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3T2Ac3JmvJB",
        "colab_type": "text"
      },
      "source": [
        "Ap√≥s estudo da tese de mestrado do cirurgi√£o dentista Mauricio Roberto Bosquiero: [DETERMINA√á√ÉO DA MATURIDADE ESQUEL√âTICA\n",
        "E ESTIMATIVA DA IDADE ATRA V√äS DE\n",
        "RADIOGRAFIAS CARPAIS.](https://storage.googleapis.com/bone-age-regression.henriquesilva.dev/docs/tese_mauricio_roberto.pdf), podemos chegar a uma terceira diretiva, referente a clara vari√¢ncia da maturidade esquel√©tica de pacientes de mesma idade e sexos opostos.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "> ### Pretendo avaliar o desempenho dos seguintes modelos pr√©-treinados: <br />\n",
        "**ResNet-50**, **Inception-V3** e **VGG-16**, a fim de criar um desenho apropriado da camada pr√©-treinada para CNNs na previs√£o da idade √≥ssea."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnYtLmx20r7J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "88ccba27-5730-4b1d-b788-d708844d4f46"
      },
      "source": [
        "# Sweetviz: Python library for Exploratory data analysis (EDA)\n",
        "!pip install sweetviz"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sweetviz in /usr/local/lib/python3.6/dist-packages (1.0a7)\n",
            "Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3 in /usr/local/lib/python3.6/dist-packages (from sweetviz) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from sweetviz) (1.18.5)\n",
            "Requirement already satisfied: importlib-resources>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from sweetviz) (3.0.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.6/dist-packages (from sweetviz) (1.4.1)\n",
            "Requirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from sweetviz) (2.11.2)\n",
            "Requirement already satisfied: tqdm>=4.43.0 in /usr/local/lib/python3.6/dist-packages (from sweetviz) (4.47.0)\n",
            "Requirement already satisfied: matplotlib>=3.1.3 in /usr/local/lib/python3.6/dist-packages (from sweetviz) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3->sweetviz) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3->sweetviz) (2018.9)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources>=1.2.0->sweetviz) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.11.1->sweetviz) (1.1.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.1.3->sweetviz) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.1.3->sweetviz) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.1.3->sweetviz) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3->sweetviz) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3mNnzLq1zZl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1389d4b8-a876-4268-861c-8b51b10b22fb"
      },
      "source": [
        "# TensorFlow e tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.image import load_img\n",
        "print(f\"TensorFlow V{tf.__version__} ü¶æ\")"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow V2.2.0 ü¶æ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6jUcKGKqvqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Libraries auxiliares\n",
        "import pandas as pd\n",
        "import sweetviz\n",
        "import requests\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import auth\n",
        "from datetime import datetime"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX8X_-MYms2U",
        "colab_type": "text"
      },
      "source": [
        "# Configurar conjuntos de desenvolvimento e de teste\n",
        "---\n",
        "\n",
        "O **Google Cloud Storage** foi escolhido como centralizador dos arquivos de suporte, bem como aos novos arquivos e relat√≥rios gerados, por oferecer a disponibilidade e a capacidade necess√°rias para se trabalhar de forma distribuida.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xf_XG-7dn5Am",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GCP_STORAGE_BASE_URI = \"https://storage.googleapis.com/bone-age-regression.henriquesilva.dev\"\n",
        "\n",
        "TRAIN_URI = f\"{GCP_STORAGE_BASE_URI}/train.csv\"\n",
        "TEST_URI = f\"{GCP_STORAGE_BASE_URI}/test.csv\"\n",
        "\n",
        "full_train_df = pd.read_csv(TRAIN_URI)\n",
        "test_df = pd.read_csv(TEST_URI)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmZHomjMw81T",
        "colab_type": "text"
      },
      "source": [
        "# Adicionar s√©ries da categoria da idade √≥ssea e link da imagem do raio-x da m√£o"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOs2EKPfYZWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_train_df['boneage_category'] = pd.cut(full_train_df['boneage'], 10)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUTOqSNlW4jN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def image_link_generator(path_uri, file_name):\n",
        "  image_link = f\"{path_uri}/images/{file_name}\"\n",
        "  return image_link\n",
        "\n",
        "full_train_df['image_link'] = full_train_df['fileName'].map(\n",
        "  lambda file_name: image_link_generator(GCP_STORAGE_BASE_URI, file_name)\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-l6mYuk1uZa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "7ab796d5-3416-463a-f2ee-c8680b6141dc"
      },
      "source": [
        "HTML(full_train_df.head(5).to_html(render_links=True, escape=False))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fileName</th>\n",
              "      <th>patientSex</th>\n",
              "      <th>boneage</th>\n",
              "      <th>boneage_category</th>\n",
              "      <th>image_link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1377.png</td>\n",
              "      <td>F</td>\n",
              "      <td>180</td>\n",
              "      <td>(159.9, 182.6]</td>\n",
              "      <td><a href=\"https://storage.googleapis.com/bone-age-regression.henriquesilva.dev/images/1377.png\" target=\"_blank\">https://storage.googleapis.com/bone-age-regression.henriquesilva.dev/images/1377.png</a></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1378.png</td>\n",
              "      <td>F</td>\n",
              "      <td>12</td>\n",
              "      <td>(0.773, 23.7]</td>\n",
              "      <td><a href=\"https://storage.googleapis.com/bone-age-regression.henriquesilva.dev/images/1378.png\" target=\"_blank\">https://storage.googleapis.com/bone-age-regression.henriquesilva.dev/images/1378.png</a></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1379.png</td>\n",
              "      <td>F</td>\n",
              "      <td>94</td>\n",
              "      <td>(91.8, 114.5]</td>\n",
              "      <td><a href=\"https://storage.googleapis.com/bone-age-regression.henriquesilva.dev/images/1379.png\" target=\"_blank\">https://storage.googleapis.com/bone-age-regression.henriquesilva.dev/images/1379.png</a></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1380.png</td>\n",
              "      <td>M</td>\n",
              "      <td>120</td>\n",
              "      <td>(114.5, 137.2]</td>\n",
              "      <td><a href=\"https://storage.googleapis.com/bone-age-regression.henriquesilva.dev/images/1380.png\" target=\"_blank\">https://storage.googleapis.com/bone-age-regression.henriquesilva.dev/images/1380.png</a></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1381.png</td>\n",
              "      <td>F</td>\n",
              "      <td>82</td>\n",
              "      <td>(69.1, 91.8]</td>\n",
              "      <td><a href=\"https://storage.googleapis.com/bone-age-regression.henriquesilva.dev/images/1381.png\" target=\"_blank\">https://storage.googleapis.com/bone-age-regression.henriquesilva.dev/images/1381.png</a></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INVRb98cu2TS",
        "colab_type": "text"
      },
      "source": [
        "# Dividir dados em treinamento e valida√ß√£o"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy4JbQX5cf1I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9473cba3-a8b2-4634-86b1-21f4ecf2b417"
      },
      "source": [
        "train_df, valid_df = train_test_split(\n",
        "  full_train_df, \n",
        "  test_size = 0.25, \n",
        "  random_state = 2018,\n",
        "  stratify = full_train_df['boneage_category']\n",
        ")\n",
        "\n",
        "print('train', train_df.shape[0], 'validation', valid_df.shape[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train 9458 validation 3153\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhL1J1XVS3Z5",
        "colab_type": "text"
      },
      "source": [
        "# An√°lise explorat√≥ria prim√°ria de dados de desenvolvimento\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MPRuqV_3wdC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = ['fileName', 'boneage_category', 'image_link']\n",
        "train = train_df.drop(columns, axis=1)\n",
        "valid = valid_df.drop(columns, axis=1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE_wnhfmT7Ca",
        "colab_type": "text"
      },
      "source": [
        "A execu√ß√£o deste comando executar√° a an√°lise e criar√° o objeto de relat√≥rio. Para obter a sa√≠da, basta usar o comando `show_html()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Kj_T3YG4Ro9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "35ed07d8-3166-455d-daae-e782caadee86"
      },
      "source": [
        "data_report = sweetviz.compare([train, \"Train\"], [valid, \"Valid\"], \"boneage\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ":FEATURES DONE:                    |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| [100%]   00:01  -> (00:00 left)\n",
            ":PAIRWISE DONE:                    |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| [100%]   00:00  -> (00:00 left)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Creating Associations graph... DONE!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAZLcbzYsH5_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "now = datetime.now()\n",
        "timestamp = now.strftime(\"%Y%m%d%H%M%S\")\n",
        "\n",
        "report_file = f\"{timestamp}_report_view.html\"\n",
        "\n",
        "data_report.show_html(report_file)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiaqrNWe5WX2",
        "colab_type": "text"
      },
      "source": [
        "# Exportar relat√≥rio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRWC_PgcsOO4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4825ac86-9844-4f3e-a0d9-ea129a96f533"
      },
      "source": [
        "auth.authenticate_user()\n",
        "\n",
        "GCP_PROJECT_ID = 'i2a2-283020'\n",
        "GCP_BUCKET_NAME = 'bone-age-regression.henriquesilva.dev'\n",
        "\n",
        "!gcloud config set project {GCP_PROJECT_ID}\n",
        "\n",
        "!gsutil cp /content/\"{report_file}\" gs://{GCP_BUCKET_NAME}/reports/\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "Copying file:///content/20200715182212_report_view.html [Content-Type=text/html]...\n",
            "-\n",
            "Operation completed over 1 objects/553.7 KiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekCm8d5bPone",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aefe2595-a517-4f07-97b4-5c97a311a621"
      },
      "source": [
        "response = requests.get(f\"{GCP_STORAGE_BASE_URI}/reports/{report_file}\")\n",
        "\n",
        "if (response.status_code == 200):\n",
        "  print(f\"Open your report it using the default browser like link: {response.url}\")\n",
        "else:\n",
        "  print(\"Sorry, the report not found.\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Open your report it using the default browser like link: https://storage.googleapis.com/bone-age-regression.henriquesilva.dev/reports/20200715182212_report_view.html\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u16rst0v6R3H",
        "colab_type": "text"
      },
      "source": [
        "# Separar os dados de desenvolvimento quanto ao sexo da paciente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kloVYQYZBTfc",
        "colab_type": "text"
      },
      "source": [
        "## train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT3VUO_X5S7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "female_train_df = train_df[(train_df.patientSex == 'F')]\n",
        "male_train_df = train_df[(train_df.patientSex == 'M')]"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5JQpYhNBV-K",
        "colab_type": "text"
      },
      "source": [
        "## valid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBP6XJNq_8qV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "female_valid_df = valid_df[(valid_df.patientSex == 'F')]\n",
        "male_valid_df = valid_df[(valid_df.patientSex == 'M')]"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usALhCQJKjdS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "880dfecc-02b8-43d4-c8d8-a9d71b39491f"
      },
      "source": [
        "# import pathlib\n",
        "\n",
        "# dataset_url = \"https://storage.googleapis.com/bone-age-regression.henriquesilva.dev/images\"\n",
        "\n",
        "# data_dir = tf.keras.utils.get_file(origin=dataset_url, \n",
        "#                                    fname='images', \n",
        "#                                    untar=True)\n",
        "\n",
        "# data_dir = pathlib.Path(data_dir)\n",
        "\n",
        "!gsutil cp --help"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mNAME\u001b[0;0m\n",
            "  cp - Copy files and objects\n",
            "\n",
            "\n",
            "\u001b[1mSYNOPSIS\u001b[0;0m\n",
            "\n",
            "  gsutil cp [OPTION]... src_url dst_url\n",
            "  gsutil cp [OPTION]... src_url... dst_url\n",
            "  gsutil cp [OPTION]... -I dst_url\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mDESCRIPTION\u001b[0;0m\n",
            "  The gsutil cp command allows you to copy data between your local file\n",
            "  system and the cloud, copy data within the cloud, and copy data between\n",
            "  cloud storage providers. For example, to upload all text files from the\n",
            "  local directory to a bucket you could do:\n",
            "\n",
            "    gsutil cp *.txt gs://my-bucket\n",
            "\n",
            "  Similarly, you can download text files from a bucket by doing:\n",
            "\n",
            "    gsutil cp gs://my-bucket/*.txt .\n",
            "\n",
            "  If you want to copy an entire directory tree you need to use the -r option.\n",
            "  For example, to upload the directory tree \"dir\":\n",
            "\n",
            "    gsutil cp -r dir gs://my-bucket\n",
            "\n",
            "  If you have a large number of files to transfer you might want to use the\n",
            "  top-level gsutil -m option (see \"gsutil help options\"), to perform a\n",
            "  parallel (multi-threaded/multi-processing) copy:\n",
            "\n",
            "    gsutil -m cp -r dir gs://my-bucket\n",
            "\n",
            "  You can pass a list of URLs (one per line) to copy on stdin instead of as\n",
            "  command line arguments by using the -I option. This allows you to use gsutil\n",
            "  in a pipeline to upload or download files / objects as generated by a program,\n",
            "  such as:\n",
            "\n",
            "    some_program | gsutil -m cp -I gs://my-bucket\n",
            "\n",
            "  or:\n",
            "\n",
            "    some_program | gsutil -m cp -I ./download_dir\n",
            "\n",
            "  The contents of stdin can name files, cloud URLs, and wildcards of files\n",
            "  and cloud URLs.\n",
            "\n",
            "  NOTE: Shells (like bash, zsh) sometimes attempt to expand wildcards in ways\n",
            "  that can be surprising. Also, attempting to copy files whose names contain\n",
            "  wildcard characters can result in problems. For more details about these\n",
            "  issues see the section \"POTENTIALLY SURPRISING BEHAVIOR WHEN USING WILDCARDS\"\n",
            "  under \"gsutil help wildcards\".\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mHOW NAMES ARE CONSTRUCTED\u001b[0;0m\n",
            "  The gsutil cp command strives to name objects in a way consistent with how\n",
            "  Linux cp works, which causes names to be constructed in varying ways depending\n",
            "  on whether you're performing a recursive directory copy or copying\n",
            "  individually named objects; and whether you're copying to an existing or\n",
            "  non-existent directory.\n",
            "\n",
            "  When performing recursive directory copies, object names are constructed that\n",
            "  mirror the source directory structure starting at the point of recursive\n",
            "  processing. For example, if dir1/dir2 contains the file a/b/c then the\n",
            "  command:\n",
            "\n",
            "    gsutil cp -r dir1/dir2 gs://my-bucket\n",
            "\n",
            "  will create the object gs://my-bucket/dir2/a/b/c.\n",
            "\n",
            "  In contrast, copying individually named files will result in objects named by\n",
            "  the final path component of the source files. For example, again assuming\n",
            "  dir1/dir2 contains a/b/c, the command:\n",
            "\n",
            "    gsutil cp dir1/dir2/** gs://my-bucket\n",
            "\n",
            "  will create the object gs://my-bucket/c.\n",
            "\n",
            "  The same rules apply for downloads: recursive copies of buckets and\n",
            "  bucket subdirectories produce a mirrored filename structure, while copying\n",
            "  individually (or wildcard) named objects produce flatly named files.\n",
            "\n",
            "  Note that in the above example the '**' wildcard matches all names\n",
            "  anywhere under dir. The wildcard '*' will match names just one level deep. For\n",
            "  more details see \"gsutil help wildcards\".\n",
            "\n",
            "  There's an additional wrinkle when working with subdirectories: the resulting\n",
            "  names depend on whether the destination subdirectory exists. For example,\n",
            "  if gs://my-bucket/subdir exists as a subdirectory, the command:\n",
            "\n",
            "    gsutil cp -r dir1/dir2 gs://my-bucket/subdir\n",
            "\n",
            "  will create the object gs://my-bucket/subdir/dir2/a/b/c. In contrast, if\n",
            "  gs://my-bucket/subdir does not exist, this same gsutil cp command will create\n",
            "  the object gs://my-bucket/subdir/a/b/c.\n",
            "\n",
            "  NOTE: If you use the\n",
            "  `Google Cloud Platform Console <https://console.cloud.google.com>`_\n",
            "  to create folders, it does so by creating a \"placeholder\" object that ends\n",
            "  with a \"/\" character. gsutil skips these objects when downloading from the\n",
            "  cloud to the local file system, because attempting to create a file that\n",
            "  ends with a \"/\" is not allowed on Linux and macOS. Because of this, it is\n",
            "  recommended that you not create objects that end with \"/\" (unless you don't\n",
            "  need to be able to download such objects using gsutil).\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mCOPYING TO/FROM SUBDIRECTORIES; DISTRIBUTING TRANSFERS ACROSS MACHINES\u001b[0;0m\n",
            "  You can use gsutil to copy to and from subdirectories by using a command\n",
            "  like:\n",
            "\n",
            "    gsutil cp -r dir gs://my-bucket/data\n",
            "\n",
            "  This will cause dir and all of its files and nested subdirectories to be\n",
            "  copied under the specified destination, resulting in objects with names like\n",
            "  gs://my-bucket/data/dir/a/b/c. Similarly you can download from bucket\n",
            "  subdirectories by using a command like:\n",
            "\n",
            "    gsutil cp -r gs://my-bucket/data dir\n",
            "\n",
            "  This will cause everything nested under gs://my-bucket/data to be downloaded\n",
            "  into dir, resulting in files with names like dir/data/a/b/c.\n",
            "\n",
            "  Copying subdirectories is useful if you want to add data to an existing\n",
            "  bucket directory structure over time. It's also useful if you want\n",
            "  to parallelize uploads and downloads across multiple machines (potentially\n",
            "  reducing overall transfer time compared with simply running gsutil -m\n",
            "  cp on one machine). For example, if your bucket contains this structure:\n",
            "\n",
            "    gs://my-bucket/data/result_set_01/\n",
            "    gs://my-bucket/data/result_set_02/\n",
            "    ...\n",
            "    gs://my-bucket/data/result_set_99/\n",
            "\n",
            "  you could perform concurrent downloads across 3 machines by running these\n",
            "  commands on each machine, respectively:\n",
            "\n",
            "    gsutil -m cp -r gs://my-bucket/data/result_set_[0-3]* dir\n",
            "    gsutil -m cp -r gs://my-bucket/data/result_set_[4-6]* dir\n",
            "    gsutil -m cp -r gs://my-bucket/data/result_set_[7-9]* dir\n",
            "\n",
            "  Note that dir could be a local directory on each machine, or it could be a\n",
            "  directory mounted off of a shared file server; whether the latter performs\n",
            "  acceptably will depend on a number of factors, so we recommend experimenting\n",
            "  to find out what works best for your computing environment.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mCOPYING IN THE CLOUD AND METADATA PRESERVATION\u001b[0;0m\n",
            "  If both the source and destination URL are cloud URLs from the same\n",
            "  provider, gsutil copies data \"in the cloud\" (i.e., without downloading\n",
            "  to and uploading from the machine where you run gsutil). In addition to\n",
            "  the performance and cost advantages of doing this, copying in the cloud\n",
            "  preserves metadata (like Content-Type and Cache-Control). In contrast,\n",
            "  when you download data from the cloud it ends up in a file, which has\n",
            "  no associated metadata. Thus, unless you have some way to hold on to\n",
            "  or re-create that metadata, downloading to a file will not retain the\n",
            "  metadata.\n",
            "\n",
            "  Copies spanning locations and/or storage classes cause data to be rewritten\n",
            "  in the cloud, which may take some time (but still will be faster than\n",
            "  downloading and re-uploading). Such operations can be resumed with the same\n",
            "  command if they are interrupted, so long as the command parameters are\n",
            "  identical.\n",
            "\n",
            "  Note that by default, the gsutil cp command does not copy the object\n",
            "  ACL to the new object, and instead will use the default bucket ACL (see\n",
            "  \"gsutil help defacl\"). You can override this behavior with the -p\n",
            "  option (see OPTIONS below).\n",
            "\n",
            "  One additional note about copying in the cloud: If the destination bucket has\n",
            "  versioning enabled, by default gsutil cp will copy only live versions of the\n",
            "  source object(s). For example:\n",
            "\n",
            "    gsutil cp gs://bucket1/obj gs://bucket2\n",
            "\n",
            "  will cause only the single live version of gs://bucket1/obj to be copied to\n",
            "  gs://bucket2, even if there are noncurrent versions of gs://bucket1/obj. To\n",
            "  also copy noncurrent versions, use the -A flag:\n",
            "\n",
            "    gsutil cp -A gs://bucket1/obj gs://bucket2\n",
            "\n",
            "  The top-level gsutil -m flag is disallowed when using the cp -A flag, to\n",
            "  ensure that version ordering is preserved.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mCHECKSUM VALIDATION\u001b[0;0m\n",
            "  At the end of every upload or download the gsutil cp command validates that\n",
            "  the checksum it computes for the source file/object matches the checksum\n",
            "  the service computes. If the checksums do not match, gsutil will delete the\n",
            "  corrupted object and print a warning message. This very rarely happens, but\n",
            "  if it does, please contact gs-team@google.com.\n",
            "\n",
            "  If you know the MD5 of a file before uploading you can specify it in the\n",
            "  Content-MD5 header, which will cause the cloud storage service to reject the\n",
            "  upload if the MD5 doesn't match the value computed by the service. For\n",
            "  example:\n",
            "\n",
            "    % gsutil hash obj\n",
            "    Hashing     obj:\n",
            "    Hashes [base64] for obj:\n",
            "            Hash (crc32c):          lIMoIw==\n",
            "            Hash (md5):             VgyllJgiiaRAbyUUIqDMmw==\n",
            "\n",
            "    % gsutil -h Content-MD5:VgyllJgiiaRAbyUUIqDMmw== cp obj gs://your-bucket/obj\n",
            "    Copying file://obj [Content-Type=text/plain]...\n",
            "    Uploading   gs://your-bucket/obj:                                182 b/182 B\n",
            "\n",
            "    If the checksum didn't match the service would instead reject the upload and\n",
            "    gsutil would print a message like:\n",
            "\n",
            "    BadRequestException: 400 Provided MD5 hash \"VgyllJgiiaRAbyUUIqDMmw==\"\n",
            "    doesn't match calculated MD5 hash \"7gyllJgiiaRAbyUUIqDMmw==\".\n",
            "\n",
            "  Even if you don't do this gsutil will delete the object if the computed\n",
            "  checksum mismatches, but specifying the Content-MD5 header has several\n",
            "  advantages:\n",
            "\n",
            "  1. It prevents the corrupted object from becoming visible at all, whereas\n",
            "     otherwise it would be visible for 1-3 seconds before gsutil deletes it.\n",
            "\n",
            "  2. If an object already exists with the given name, specifying the\n",
            "     Content-MD5 header will cause the existing object never to be replaced,\n",
            "     whereas otherwise it would be replaced by the corrupted object and then\n",
            "     deleted a few seconds later.\n",
            "\n",
            "  3. It will definitively prevent the corrupted object from being left in\n",
            "     the cloud, whereas the gsutil approach of deleting after the upload\n",
            "     completes could fail if (for example) the gsutil process gets ^C'd\n",
            "     between upload and deletion request.\n",
            "\n",
            "  4. It supports a customer-to-service integrity check handoff. For example,\n",
            "     if you have a content production pipeline that generates data to be\n",
            "     uploaded to the cloud along with checksums of that data, specifying the\n",
            "     MD5 computed by your content pipeline when you run gsutil cp will ensure\n",
            "     that the checksums match all the way through the process (e.g., detecting\n",
            "     if data gets corrupted on your local disk between the time it was written\n",
            "     by your content pipeline and the time it was uploaded to Google Cloud\n",
            "     Storage).\n",
            "\n",
            "  NOTE: The Content-MD5 header is ignored for composite objects, because such\n",
            "  objects only have a CRC32C checksum.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mRETRY HANDLING\u001b[0;0m\n",
            "  The cp command will retry when failures occur, but if enough failures happen\n",
            "  during a particular copy or delete operation the cp command will skip that\n",
            "  object and move on. At the end of the copy run if any failures were not\n",
            "  successfully retried, the cp command will report the count of failures, and\n",
            "  exit with non-zero status.\n",
            "\n",
            "  Note that there are cases where retrying will never succeed, such as if you\n",
            "  don't have write permission to the destination bucket or if the destination\n",
            "  path for some objects is longer than the maximum allowed length.\n",
            "\n",
            "  For more details about gsutil's retry handling, please see\n",
            "  \"gsutil help retries\".\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mRESUMABLE TRANSFERS\u001b[0;0m\n",
            "  gsutil automatically performs a resumable upload whenever you use the cp\n",
            "  command to upload an object that is larger than 8 MiB. You do not need to\n",
            "  specify any special command line options to make this happen. If your upload\n",
            "  is interrupted you can restart the upload by running the same cp command that\n",
            "  you ran to start the upload. Until the upload has completed successfully, it\n",
            "  will not be visible at the destination object and will not replace any\n",
            "  existing object the upload is intended to overwrite. However, see the section\n",
            "  on parallel composite uploads, which may leave temporary component objects in\n",
            "  place during the upload process.\n",
            "\n",
            "  Similarly, gsutil automatically performs resumable downloads (using standard\n",
            "  HTTP Range GET operations) whenever you use the cp command, unless the\n",
            "  destination is a stream. In this case, a partially downloaded temporary file\n",
            "  will be visible in the destination directory. Upon completion, the original\n",
            "  file is deleted and overwritten with the downloaded contents.\n",
            "\n",
            "  Resumable uploads and downloads store state information in files under\n",
            "  ~/.gsutil, named by the destination object or file. If you attempt to resume a\n",
            "  transfer from a machine with a different directory, the transfer will start\n",
            "  over from scratch.\n",
            "\n",
            "  See also \"gsutil help prod\" for details on using resumable transfers\n",
            "  in production.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mSTREAMING TRANSFERS\u001b[0;0m\n",
            "  Use '-' in place of src_url or dst_url to perform a streaming\n",
            "  transfer. For example:\n",
            "\n",
            "    long_running_computation | gsutil cp - gs://my-bucket/obj\n",
            "\n",
            "  Streaming uploads using the JSON API (see \"gsutil help apis\") are buffered in\n",
            "  memory part-way back into the file and can thus retry in the event of network\n",
            "  or service problems.\n",
            "\n",
            "  Streaming transfers using the XML API do not support resumable\n",
            "  uploads/downloads. If you have a large amount of data to upload (say, more\n",
            "  than 100 MiB) it is recommended that you write the data to a local file and\n",
            "  then copy that file to the cloud rather than streaming it (and similarly for\n",
            "  large downloads).\n",
            "\n",
            "  CAUTION: When performing a streaming transfer to or from Cloud Storage,\n",
            "  neither Cloud Storage nor gsutil compute a checksum. If you require data\n",
            "  validation, use a non-streaming transfer, which performs integrity checking\n",
            "  automatically.\n",
            "\n",
            "  NOTE: Streaming transfers are not allowed when the top-level gsutil -m flag\n",
            "  is used.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mSLICED OBJECT DOWNLOADS\u001b[0;0m\n",
            "  gsutil uses HTTP Range GET requests to perform \"sliced\" downloads in parallel\n",
            "  when downloading large objects from Google Cloud Storage. This means that disk\n",
            "  space for the temporary download destination file will be pre-allocated and\n",
            "  byte ranges (slices) within the file will be downloaded in parallel. Once all\n",
            "  slices have completed downloading, the temporary file will be renamed to the\n",
            "  destination file. No additional local disk space is required for this\n",
            "  operation.\n",
            "\n",
            "  This feature is only available for Google Cloud Storage objects because it\n",
            "  requires a fast composable checksum (CRC32C) that can be used to verify the\n",
            "  data integrity of the slices. And because it depends on CRC32C, using sliced\n",
            "  object downloads also requires a compiled crcmod (see \"gsutil help crcmod\") on\n",
            "  the machine performing the download. If compiled crcmod is not available,\n",
            "  a non-sliced object download will instead be performed.\n",
            "\n",
            "  NOTE: since sliced object downloads cause multiple writes to occur at various\n",
            "  locations on disk, this mechanism can degrade performance for disks with slow\n",
            "  seek times, especially for large numbers of slices. While the default number\n",
            "  of slices is set small to avoid this problem, you can disable sliced object\n",
            "  download if necessary by setting the \"sliced_object_download_threshold\"\n",
            "  variable in the .boto config file to 0.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mPARALLEL COMPOSITE UPLOADS\u001b[0;0m\n",
            "  gsutil can automatically use\n",
            "  `object composition <https://cloud.google.com/storage/docs/composite-objects>`_\n",
            "  to perform uploads in parallel for large, local files being uploaded to Google\n",
            "  Cloud Storage. If enabled (see below), a large file will be split into\n",
            "  component pieces that are uploaded in parallel and then composed in the cloud\n",
            "  (and the temporary components finally deleted). A file can be broken into as\n",
            "  many as 32 component pieces; until this piece limit is reached, the maximum\n",
            "  size of each component piece is determined by the variable\n",
            "  \"parallel_composite_upload_component_size,\" specified in the [GSUtil] section\n",
            "  of your .boto configuration file (for files that are otherwise too big,\n",
            "  components are as large as needed to fit into 32 pieces). No additional local\n",
            "  disk space is required for this operation.\n",
            "\n",
            "  Using parallel composite uploads presents a tradeoff between upload\n",
            "  performance and download configuration: If you enable parallel composite\n",
            "  uploads your uploads will run faster, but someone will need to install a\n",
            "  compiled crcmod (see \"gsutil help crcmod\") on every machine where objects are\n",
            "  downloaded by gsutil or other Python applications. Note that for such uploads,\n",
            "  crcmod is required for downloading regardless of whether the parallel\n",
            "  composite upload option is on or not. For some distributions this is easy\n",
            "  (e.g., it comes pre-installed on macOS), but in other cases some users have\n",
            "  found it difficult. Because of this, at present parallel composite uploads are\n",
            "  disabled by default. Google is actively working with a number of the Linux\n",
            "  distributions to get crcmod included with the stock distribution. Once that is\n",
            "  done we will re-enable parallel composite uploads by default in gsutil.\n",
            "\n",
            "  WARNING: Parallel composite uploads should not be used with NEARLINE,\n",
            "  COLDLINE, or ARCHIVE storage class buckets, because doing so incurs an early\n",
            "  deletion charge for each component object.\n",
            "  \n",
            "  WARNING: Parallel composite uploads should not be used in buckets that have a\n",
            "  `retention policy <https://cloud.google.com/storage/docs/bucket-lock>`_,\n",
            "  because the component pieces cannot be deleted until each has met the\n",
            "  bucket's minimum retention period.\n",
            "\n",
            "  To try parallel composite uploads you can run the command:\n",
            "\n",
            "    gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp bigfile gs://your-bucket\n",
            "\n",
            "  where bigfile is larger than 150 MiB. When you do this, notice that the upload\n",
            "  progress indicator continuously updates for the file, until all parts of the\n",
            "  upload complete. If after trying this you want to enable parallel composite\n",
            "  uploads for all of your future uploads (notwithstanding the caveats mentioned\n",
            "  earlier), you can uncomment and set the \"parallel_composite_upload_threshold\"\n",
            "  config value in your .boto configuration file to this value.\n",
            "\n",
            "  Note that the crcmod problem only impacts downloads via Python applications\n",
            "  (such as gsutil). If all users who need to download the data using gsutil or\n",
            "  other Python applications can install crcmod, or if no Python users will\n",
            "  need to download your objects, it makes sense to enable parallel composite\n",
            "  uploads (see above). For example, if you use gsutil to upload video assets,\n",
            "  and those assets will only ever be served via a Java application, it would\n",
            "  make sense to enable parallel composite uploads on your machine (there are\n",
            "  efficient CRC32C implementations available in Java).\n",
            "\n",
            "  If a parallel composite upload fails prior to composition, re-running the\n",
            "  gsutil command will take advantage of resumable uploads for the components\n",
            "  that failed, and the component objects will be deleted after the first\n",
            "  successful attempt. Any temporary objects that were uploaded successfully\n",
            "  before gsutil failed will still exist until the upload is completed\n",
            "  successfully. The temporary objects will be named in the following fashion:\n",
            "\n",
            "    <random ID>/gsutil/tmp/parallel_composite_uploads/for_details_see/gsutil_help_cp/<hash>\n",
            "\n",
            "  where <random ID> is a numerical value, and <hash> is an MD5 hash (not related\n",
            "  to the hash of the contents of the file or object).\n",
            "\n",
            "  To avoid leaving temporary objects around, you should make sure to check the\n",
            "  exit status from the gsutil command.  This can be done in a bash script, for\n",
            "  example, by doing:\n",
            "\n",
            "    if ! gsutil cp ./local-file gs://your-bucket/your-object; then\n",
            "      << Code that handles failures >>\n",
            "    fi\n",
            "\n",
            "  Or, for copying a directory, use this instead:\n",
            "\n",
            "    if ! gsutil cp -c -L cp.log -r ./dir gs://bucket; then\n",
            "      << Code that handles failures >>\n",
            "    fi\n",
            "\n",
            "  Note that an object uploaded using parallel composite uploads will have a\n",
            "  CRC32C hash, but it will not have an MD5 hash (and because of that, users who\n",
            "  download the object must have crcmod installed, as noted earlier). For details\n",
            "  see \"gsutil help crc32c\".\n",
            "\n",
            "  Parallel composite uploads can be disabled by setting the\n",
            "  \"parallel_composite_upload_threshold\" variable in the .boto config file to 0.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mCHANGING TEMP DIRECTORIES\u001b[0;0m\n",
            "  gsutil writes data to a temporary directory in several cases:\n",
            "\n",
            "  - when compressing data to be uploaded (see the -z and -Z options)\n",
            "  - when decompressing data being downloaded (when the data has\n",
            "    Content-Encoding:gzip, e.g., as happens when uploaded using gsutil cp -z\n",
            "    or gsutil cp -Z)\n",
            "  - when running integration tests (using the gsutil test command)\n",
            "\n",
            "  In these cases it's possible the temp file location on your system that\n",
            "  gsutil selects by default may not have enough space. If gsutil runs out of\n",
            "  space during one of these operations (e.g., raising\n",
            "  \"CommandException: Inadequate temp space available to compress <your file>\"\n",
            "  during a gsutil cp -z operation), you can change where it writes these\n",
            "  temp files by setting the TMPDIR environment variable. On Linux and macOS\n",
            "  you can do this either by running gsutil this way:\n",
            "\n",
            "    TMPDIR=/some/directory gsutil cp ...\n",
            "\n",
            "  or by adding this line to your ~/.bashrc file and then restarting the shell\n",
            "  before running gsutil:\n",
            "\n",
            "    export TMPDIR=/some/directory\n",
            "\n",
            "  On Windows 7 you can change the TMPDIR environment variable from Start ->\n",
            "  Computer -> System -> Advanced System Settings -> Environment Variables.\n",
            "  You need to reboot after making this change for it to take effect. (Rebooting\n",
            "  is not necessary after running the export command on Linux and macOS.)\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mSYNCHRONIZING OVER OS-SPECIFIC FILE TYPES (SYMLINKS, DEVICES, ETC.)\u001b[0;0m\n",
            "\n",
            "  Please see the section about OS-specific file types in \"gsutil help rsync\".\n",
            "  While that section was written specifically about the rsync command, analogous\n",
            "  points apply to the cp command.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mOPTIONS\u001b[0;0m\n",
            "  -a canned_acl  Sets named canned_acl when uploaded objects created. See\n",
            "                 \"gsutil help acls\" for further details.\n",
            "\n",
            "  -A             Copy all source versions from a source buckets/folders.\n",
            "                 If not set, only the live version of each source object is\n",
            "                 copied.\n",
            "                 \n",
            "                 NOTE: this option is only useful when the destination\n",
            "                 bucket has versioning enabled.\n",
            "\n",
            "  -c             If an error occurs, continue to attempt to copy the remaining\n",
            "                 files. If any copies were unsuccessful, gsutil's exit status\n",
            "                 will be non-zero even if this flag is set. This option is\n",
            "                 implicitly set when running \"gsutil -m cp...\".\n",
            "                 \n",
            "                 NOTE: -c only applies to the actual copying operation. If an\n",
            "                 error occurs while iterating over the files in the local\n",
            "                 directory (e.g., invalid Unicode file name) gsutil will print\n",
            "                 an error message and abort.\n",
            "\n",
            "  -D             Copy in \"daisy chain\" mode, i.e., copying between two buckets\n",
            "                 by hooking a download to an upload, via the machine where\n",
            "                 gsutil is run. This stands in contrast to the default, where\n",
            "                 data are copied between two buckets \"in the cloud\", i.e.,\n",
            "                 without needing to copy via the machine where gsutil runs.\n",
            "\n",
            "                 By default, a \"copy in the cloud\" when the source is a\n",
            "                 composite object will retain the composite nature of the\n",
            "                 object. However, Daisy chain mode can be used to change a\n",
            "                 composite object into a non-composite object. For example:\n",
            "\n",
            "                     gsutil cp -D -p gs://bucket/obj gs://bucket/obj_tmp\n",
            "                     gsutil mv -p gs://bucket/obj_tmp gs://bucket/obj\n",
            "\n",
            "                 NOTE: Daisy chain mode is automatically used when copying\n",
            "                 between providers (e.g., to copy data from Google Cloud Storage\n",
            "                 to another provider).\n",
            "\n",
            "  -e             Exclude symlinks. When specified, symbolic links will not be\n",
            "                 copied.\n",
            "\n",
            "  -I             Causes gsutil to read the list of files or objects to copy from\n",
            "                 stdin. This allows you to run a program that generates the list\n",
            "                 of files to upload/download.\n",
            "\n",
            "  -j <ext,...>   Applies gzip transport encoding to any file upload whose\n",
            "                 extension matches the -j extension list. This is useful when\n",
            "                 uploading files with compressible content (such as .js, .css,\n",
            "                 or .html files) because it saves network bandwidth while\n",
            "                 also leaving the data uncompressed in Google Cloud Storage.\n",
            "\n",
            "                 When you specify the -j option, files being uploaded are\n",
            "                 compressed in-memory and on-the-wire only. Both the local\n",
            "                 files and Cloud Storage objects remain uncompressed. The\n",
            "                 uploaded objects retain the Content-Type and name of the\n",
            "                 original files.\n",
            "\n",
            "                 Note that if you want to use the top-level -m option to\n",
            "                 parallelize copies along with the -j/-J options, your\n",
            "                 performance may be bottlenecked by the\n",
            "                 \"max_upload_compression_buffer_size\" boto config option,\n",
            "                 which is set to 2 GiB by default. This compression buffer\n",
            "                 size can be changed to a higher limit, e.g.:\n",
            "\n",
            "                   gsutil -o \"GSUtil:max_upload_compression_buffer_size=8G\" \\\n",
            "                     -m cp -j html -r /local/source/dir gs://bucket/path\n",
            "\n",
            "  -J             Applies gzip transport encoding to file uploads. This option\n",
            "                 works like the -j option described above, but it applies to\n",
            "                 all uploaded files, regardless of extension.\n",
            "\n",
            "                 CAUTION: If you use this option and some of the source files\n",
            "                 don't compress well (e.g., that's often true of binary data),\n",
            "                 this option may result in longer uploads.\n",
            "\n",
            "  -L <file>      Outputs a manifest log file with detailed information about\n",
            "                 each item that was copied. This manifest contains the following\n",
            "                 information for each item:\n",
            "\n",
            "                 - Source path.\n",
            "                 - Destination path.\n",
            "                 - Source size.\n",
            "                 - Bytes transferred.\n",
            "                 - MD5 hash.\n",
            "                 - UTC date and time transfer was started in ISO 8601 format.\n",
            "                 - UTC date and time transfer was completed in ISO 8601 format.\n",
            "                 - Upload id, if a resumable upload was performed.\n",
            "                 - Final result of the attempted transfer, success or failure.\n",
            "                 - Failure details, if any.\n",
            "\n",
            "                 If the log file already exists, gsutil will use the file as an\n",
            "                 input to the copy process, and will also append log items to\n",
            "                 the existing file. Files/objects that are marked in the\n",
            "                 existing log file as having been successfully copied (or\n",
            "                 skipped) will be ignored. Files/objects without entries will be\n",
            "                 copied and ones previously marked as unsuccessful will be\n",
            "                 retried. This can be used in conjunction with the -c option to\n",
            "                 build a script that copies a large number of objects reliably,\n",
            "                 using a bash script like the following:\n",
            "\n",
            "                   until gsutil cp -c -L cp.log -r ./dir gs://bucket; do\n",
            "                     sleep 1\n",
            "                   done\n",
            "\n",
            "                 The -c option will cause copying to continue after failures\n",
            "                 occur, and the -L option will allow gsutil to pick up where it\n",
            "                 left off without duplicating work. The loop will continue\n",
            "                 running as long as gsutil exits with a non-zero status (such a\n",
            "                 status indicates there was at least one failure during the\n",
            "                 gsutil run).\n",
            "\n",
            "                 NOTE: If you're trying to synchronize the contents of a\n",
            "                 directory and a bucket (or two buckets), see\n",
            "                 \"gsutil help rsync\".\n",
            "\n",
            "  -n             No-clobber. When specified, existing files or objects at the\n",
            "                 destination will not be overwritten. Any items that are skipped\n",
            "                 by this option will be reported as being skipped. This option\n",
            "                 will perform an additional GET request to check if an item\n",
            "                 exists before attempting to upload the data. This will save\n",
            "                 retransmitting data, but the additional HTTP requests may make\n",
            "                 small object transfers slower and more expensive.\n",
            "\n",
            "  -p             Causes ACLs to be preserved when copying in the cloud. Note\n",
            "                 that this option has performance and cost implications when\n",
            "                 using  the XML API, as it requires separate HTTP calls for\n",
            "                 interacting with ACLs. (There are no such performance or cost\n",
            "                 implications when using the -p option with the JSON API.) The\n",
            "                 performance issue can be mitigated to some degree by using\n",
            "                 gsutil -m cp to cause parallel copying. Note that this option\n",
            "                 only works if you have OWNER access to all of the objects that\n",
            "                 are copied.\n",
            "\n",
            "                 You can avoid the additional performance and cost of using\n",
            "                 cp -p if you want all objects in the destination bucket to end\n",
            "                 up with the same ACL by setting a default object ACL on that\n",
            "                 bucket instead of using cp -p. See \"gsutil help defacl\".\n",
            "\n",
            "                 Note that it's not valid to specify both the -a and -p options\n",
            "                 together.\n",
            "\n",
            "  -P             Causes POSIX attributes to be preserved when objects are\n",
            "                 copied. With this feature enabled, gsutil cp will copy fields\n",
            "                 provided by stat. These are the user ID of the owner, the group\n",
            "                 ID of the owning group, the mode (permissions) of the file, and\n",
            "                 the access/modification time of the file. For downloads, these\n",
            "                 attributes will only be set if the source objects were uploaded\n",
            "                 with this flag enabled.\n",
            "\n",
            "                 On Windows, this flag will only set and restore access time and\n",
            "                 modification time. This is because Windows doesn't have a\n",
            "                 notion of POSIX uid/gid/mode.\n",
            "\n",
            "  -R, -r         The -R and -r options are synonymous. Causes directories,\n",
            "                 buckets, and bucket subdirectories to be copied recursively.\n",
            "                 If you neglect to use this option for an upload, gsutil will\n",
            "                 copy any files it finds and skip any directories. Similarly,\n",
            "                 neglecting to specify this option for a download will cause\n",
            "                 gsutil to copy any objects at the current bucket directory\n",
            "                 level, and skip any subdirectories.\n",
            "\n",
            "  -s <class>     The storage class of the destination object(s). If not\n",
            "                 specified, the default storage class of the destination bucket\n",
            "                 is used. Not valid for copying to non-cloud destinations.\n",
            "\n",
            "  -U             Skip objects with unsupported object types instead of failing.\n",
            "                 Unsupported object types are Amazon S3 Objects in the GLACIER\n",
            "                 storage class.\n",
            "\n",
            "  -v             Requests that the version-specific URL for each uploaded object\n",
            "                 be printed. Given this URL you can make future upload requests\n",
            "                 that are safe in the face of concurrent updates, because Google\n",
            "                 Cloud Storage will refuse to perform the update if the current\n",
            "                 object version doesn't match the version-specific URL. See\n",
            "                 \"gsutil help versions\" for more details.\n",
            "\n",
            "  -z <ext,...>   Applies gzip content-encoding to any file upload whose\n",
            "                 extension matches the -z extension list. This is useful when\n",
            "                 uploading files with compressible content (such as .js, .css,\n",
            "                 or .html files) because it saves network bandwidth and space\n",
            "                 in Google Cloud Storage, which in turn reduces storage costs.\n",
            "\n",
            "                 When you specify the -z option, the data from your files is\n",
            "                 compressed before it is uploaded, but your actual files are\n",
            "                 left uncompressed on the local disk. The uploaded objects\n",
            "                 retain the Content-Type and name of the original files but are\n",
            "                 given a Content-Encoding header with the value \"gzip\" to\n",
            "                 indicate that the object data stored are compressed on the\n",
            "                 Google Cloud Storage servers.\n",
            "\n",
            "                 For example, the following command:\n",
            "\n",
            "                   gsutil cp -z html -a public-read \\\n",
            "                     cattypes.html tabby.jpeg gs://mycats\n",
            "\n",
            "                 will do all of the following:\n",
            "\n",
            "                 - Upload the files cattypes.html and tabby.jpeg to the bucket\n",
            "                   gs://mycats (cp command)\n",
            "                 - Set the Content-Type of cattypes.html to text/html and\n",
            "                   tabby.jpeg to image/jpeg (based on file extensions)\n",
            "                 - Compress the data in the file cattypes.html (-z option)\n",
            "                 - Set the Content-Encoding for cattypes.html to gzip\n",
            "                   (-z option)\n",
            "                 - Set the ACL for both files to public-read (-a option)\n",
            "                 - If a user tries to view cattypes.html in a browser, the\n",
            "                   browser will know to uncompress the data based on the\n",
            "                   Content-Encoding header and to render it as HTML based on\n",
            "                   the Content-Type header.\n",
            "\n",
            "                 Because the -z/-Z options compress data prior to upload, they\n",
            "                 are not subject to the same compression buffer bottleneck that\n",
            "                 can affect the -j/-J options.\n",
            "\n",
            "                 Note that if you download an object with Content-Encoding:gzip\n",
            "                 gsutil decompresses the content before writing the local file.\n",
            "\n",
            "  -Z             Applies gzip content-encoding to file uploads. This option\n",
            "                 works like the -z option described above, but it applies to\n",
            "                 all uploaded files, regardless of extension.\n",
            "\n",
            "                 CAUTION: If you use this option and some of the source files\n",
            "                 don't compress well (e.g., that's often true of binary data),\n",
            "                 this option may result in files taking up more space in the\n",
            "                 cloud than they would if left uncompressed."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyboeZoXCPd4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "357bb9d3-1652-42aa-c8f7-5f4cf106ce77"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "IMG_SIZE = (384, 384) # slightly smaller than vgg16 normally expects\n",
        "core_idg = ImageDataGenerator(samplewise_center=False, \n",
        "                              samplewise_std_normalization=False, \n",
        "                              horizontal_flip = True, \n",
        "                              vertical_flip = False, \n",
        "                              height_shift_range = 0.15, \n",
        "                              width_shift_range = 0.15, \n",
        "                              rotation_range = 5, \n",
        "                              shear_range = 0.01,\n",
        "                              fill_mode = 'nearest',\n",
        "                              zoom_range=0.25,\n",
        "                             preprocessing_function = preprocess_input)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxBjV9MJHweK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXSIme50EAh6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flow_from_dataframe(img_data_gen, in_df, path_col, y_col, **dflow_args):\n",
        "    # base_dir = in_df[path_col].value\n",
        "    print('## Ignore next message from keras, values are replaced anyways')\n",
        "    df_gen = img_data_gen.flow_from_directory(base_dir, \n",
        "                                     class_mode = 'sparse',\n",
        "                                    **dflow_args)\n",
        "    df_gen.filenames = in_df[path_col].values\n",
        "    df_gen.classes = np.stack(in_df[y_col].values)\n",
        "    df_gen.samples = in_df.shape[0]\n",
        "    df_gen.n = in_df.shape[0]\n",
        "    df_gen._set_index_array()\n",
        "    df_gen.directory = '' # since we have the full path\n",
        "    print('Reinserting dataframe: {} images'.format(in_df.shape[0]))\n",
        "    return df_gen"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "310OSnYOEzdr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "41359867-eae5-461e-d50d-5a77d6858337"
      },
      "source": [
        "!gsutil ls gs://{GCP_BUCKET_NAME}/reports/"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://bone-age-regression.henriquesilva.dev/reports/\n",
            "gs://bone-age-regression.henriquesilva.dev/reports/20200714002253_report_view.html\n",
            "gs://bone-age-regression.henriquesilva.dev/reports/20200715013044_report_view.html\n",
            "gs://bone-age-regression.henriquesilva.dev/reports/20200715013336_report_view.html\n",
            "gs://bone-age-regression.henriquesilva.dev/reports/20200715180959_report_view.html\n",
            "gs://bone-age-regression.henriquesilva.dev/reports/20200715182212_report_view.html\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsT_4VKqEHuy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "47b44c9e-53af-423e-e5f7-3ec9442f1543"
      },
      "source": [
        "train_gen = flow_from_dataframe(core_idg, train_df, \n",
        "                             path_col = 'image_link',\n",
        "                            y_col = 'boneage_zscore', \n",
        "                            target_size = IMG_SIZE,\n",
        "                             color_mode = 'rgb',\n",
        "                            batch_size = 32)\n",
        "\n",
        "valid_gen = flow_from_dataframe(core_idg, valid_df, \n",
        "                             path_col = 'image_link',\n",
        "                            y_col = 'boneage_zscore', \n",
        "                            target_size = IMG_SIZE,\n",
        "                             color_mode = 'rgb',\n",
        "                            batch_size = 256) # we can use much larger batches for evaluation\n",
        "# used a fixed dataset for evaluating the algorithm\n",
        "test_X, test_Y = next(flow_from_dataframe(core_idg, \n",
        "                               valid_df, \n",
        "                             path_col = 'image_link',\n",
        "                            y_col = 'boneage_zscore', \n",
        "                            target_size = IMG_SIZE,\n",
        "                             color_mode = 'rgb',\n",
        "                            batch_size = 1024)) # one big batch"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "## Ignore next message from keras, values are replaced anyways\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-7e4639b9a97c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                             \u001b[0mtarget_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                              \u001b[0mcolor_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'rgb'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                             batch_size = 32)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m valid_gen = flow_from_dataframe(core_idg, valid_df, \n",
            "\u001b[0;32m<ipython-input-80-8e5a552d46cb>\u001b[0m in \u001b[0;36mflow_from_dataframe\u001b[0;34m(img_data_gen, in_df, path_col, y_col, **dflow_args)\u001b[0m\n\u001b[1;32m      4\u001b[0m     df_gen = img_data_gen.flow_from_directory(base_dir, \n\u001b[1;32m      5\u001b[0m                                      \u001b[0mclass_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sparse'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                                     **dflow_args)\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdf_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdf_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0mfollow_links\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_links\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m             \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         )\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             dtype=dtype)\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/directory_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'https://storage.googleapis.com/bone-age-regression.henriquesilva.dev/images/3726.png'"
          ]
        }
      ]
    }
  ]
}